{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b90252-1533-4ec4-b537-319f4dacc770",
   "metadata": {},
   "source": [
    "We strongly encourage users to perform model training using our script `tissuenarrator/train.py`.  \n",
    "However, a Jupyter notebook version is also provided for exploratory and demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f2b3ff-3dfd-4b16-96ec-f9bc0398378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, Features, Sequence, Value, load_from_disk\n",
    "from unsloth import FastLanguageModel, UnslothTrainer, UnslothTrainingArguments\n",
    "from transformers import TrainerCallback, TrainerControl, TrainerState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee7c9ccf-9791-4649-b0db-1e8c39c281a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.9: Fast Qwen3 patching. Transformers: 4.55.2. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 1. Max memory: 47.428 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=32000\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-Base\",\n",
    "    max_seq_length = max_seq_length,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = False,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,     # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False,  # We have full finetuning now!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de7545d9-c47a-49ef-aa2d-bd1ce59f6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d35ee5d-9e63-4d74-bc56-c5e913f1fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/home/sizheliu/spatial-text/data/merfish/merfish_all_spatial_df.parquet\").head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab75af-8016-45c2-96eb-db921c370fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "COORD_RE = re.compile(r'(X|Y):\\s*-?\\d+(?:\\.\\d+)?')\n",
    "\n",
    "def split_and_mask(\n",
    "    text,\n",
    "    tokenizer,\n",
    "    max_seq_length=32000,\n",
    "    overlap=2,          # number of sentences to repeat between chunks\n",
    "    min_length=100        # minimum token count to keep a chunk (0 = keep all)\n",
    "):\n",
    "    sentences = re.findall(r\"<pos>.*?</cs>\", text, flags=re.DOTALL)\n",
    "\n",
    "    results = []\n",
    "    i = 0\n",
    "    n = len(sentences)\n",
    "\n",
    "    while i < n:\n",
    "        start = max(0, i - overlap)\n",
    "        current_chunk = []\n",
    "        token_count = 0\n",
    "        j = start\n",
    "\n",
    "        while j < n:\n",
    "            sent = sentences[j]\n",
    "            tok_ids = tokenizer(sent, add_special_tokens=False)[\"input_ids\"]\n",
    "            new_count = token_count + len(tok_ids)\n",
    "            if new_count > max_seq_length:\n",
    "                break\n",
    "            current_chunk.append(sent)\n",
    "            token_count = new_count\n",
    "            j += 1\n",
    "\n",
    "        chunk_text = \" \".join(current_chunk) if current_chunk else \"\"\n",
    "\n",
    "        if chunk_text:\n",
    "            enc_len = len(tokenizer(chunk_text, add_special_tokens=False)[\"input_ids\"])\n",
    "            if min_length == 0 or enc_len >= min_length:\n",
    "                # mask XY numbers in labels (do not train to predict coords)\n",
    "                enc = tokenizer(\n",
    "                    chunk_text,\n",
    "                    return_offsets_mapping=True,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True,\n",
    "                    max_length=max_seq_length,\n",
    "                )\n",
    "                input_ids = enc[\"input_ids\"]\n",
    "                offsets = enc[\"offset_mapping\"]\n",
    "                labels = input_ids.copy()\n",
    "\n",
    "                # find char spans to mask\n",
    "                mask_spans = []\n",
    "                for m in COORD_RE.finditer(chunk_text):\n",
    "                    full_start, full_end = m.span()\n",
    "                    match_str = m.group()\n",
    "                    # find where the number starts inside the match\n",
    "                    num_start = match_str.find(\":\") + 2\n",
    "                    # adjust absolute positions\n",
    "                    span_start = full_start + num_start\n",
    "                    span_end = full_end\n",
    "                    mask_spans.append((span_start, span_end))\n",
    "                    \n",
    "                for k, (s_char, e_char) in enumerate(offsets):\n",
    "                    for a, b in mask_spans:\n",
    "                        if s_char >= a and e_char <= b:\n",
    "                            labels[k] = -100\n",
    "                            break\n",
    "\n",
    "\n",
    "                results.append({\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"labels\": labels,\n",
    "                    \"attention_mask\": [1] * len(input_ids),\n",
    "                })\n",
    "\n",
    "        if j >= n:\n",
    "            i = n\n",
    "        else:\n",
    "            i = max(j - overlap + 1, 0)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe310214-32ea-42a1-928e-83e8865ffabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting & Masking: 100%|██████████| 100/100 [00:18<00:00,  5.41it/s]\n"
     ]
    }
   ],
   "source": [
    "all_records = []\n",
    "for sent, split in tqdm(zip(df[\"sentence\"], df[\"split\"]), total=len(df), desc=\"Splitting & Masking\"):\n",
    "    chunks = split_and_mask(sent, tokenizer, max_seq_length=max_seq_length)\n",
    "    for c in chunks:\n",
    "        c[\"split\"] = split\n",
    "        all_records.append(c)\n",
    "\n",
    "features = Features({\n",
    "    \"input_ids\": Sequence(Value(\"int32\")),\n",
    "    \"labels\": Sequence(Value(\"int32\")),\n",
    "    \"attention_mask\": Sequence(Value(\"int8\")),\n",
    "    \"split\": Value(\"string\"),\n",
    "})\n",
    "\n",
    "hf_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [r[\"input_ids\"] for r in all_records],\n",
    "    \"labels\": [r[\"labels\"] for r in all_records],\n",
    "    \"attention_mask\": [r[\"attention_mask\"] for r in all_records],\n",
    "    \"split\": [r[\"split\"] for r in all_records],\n",
    "}, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be082c5f-52eb-409b-8af0-6e71028cf03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 142/142 [00:02<00:00, 49.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac0e856f-633e-42be-bbe9-efed067926ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "   model = model,\n",
    "   tokenizer = tokenizer, \n",
    "   train_dataset = train_dataset,\n",
    "   max_seq_length = max_seq_length,\n",
    "   dataset_num_proc = 2,\n",
    "   args = UnslothTrainingArguments(\n",
    "       per_device_train_batch_size = 2,\n",
    "       gradient_accumulation_steps = 8,\n",
    "       warmup_ratio = 0.01,\n",
    "       num_train_epochs = 1,\n",
    "       # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "       learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "       logging_steps = 100,\n",
    "       optim = \"adamw_8bit\",\n",
    "       weight_decay = 0.01,\n",
    "       lr_scheduler_type = \"cosine\",\n",
    "       seed = 3407,\n",
    "       output_dir = \"./test_train\",\n",
    "       report_to = \"none\", # Use this for WandB etc\n",
    "       # ⬇️ Checkpoint config\n",
    "       save_strategy = \"steps\",\n",
    "       save_steps = 500,\n",
    "       save_total_limit = 50,\n",
    "       logging_strategy = \"steps\",\n",
    "   ),\n",
    ")\n",
    "stats = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
